{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ***Packages***\n###### ***Ignore the errors***","metadata":{}},{"cell_type":"code","source":"# To ensure compatiblity of TensorFlow-Addons with TensorFlow\n!pip install tensorflow-addons[tensorflow]","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport time\nimport glob\nimport scipy\nimport random\nimport shutil\nimport pathlib\nimport zipfile\nimport requests\nimport datetime\nimport tifffile\nimport platform\nimport ipykernel\nimport itertools\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport seaborn as sns\nfrom skimage import io\nimport tensorflow as tf\nfrom keras.models import *\nfrom keras.layers import *\nfrom IPython import display\nfrom tensorflow import keras\nfrom skimage import io, color\nfrom keras.models import Model\nfrom keras import backend as K\nimport matplotlib.pyplot as plt\nimport tensorflow_addons as tfa\nfrom matplotlib.patches import Patch\nfrom skimage.transform import resize\nfrom skimage.io import imread, imsave\nfrom keras.utils import to_categorical\nfrom sklearn.utils import class_weight\nfrom matplotlib.colors import Normalize\nfrom skimage.util import view_as_windows\nfrom keras.metrics import Precision, Recall\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.models import load_model\nfrom matplotlib.patches import Rectangle, Patch\nfrom scipy.ndimage import distance_transform_edt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import LambdaCallback\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras import layers, models, regularizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint\nfrom sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\nfrom tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, UpSampling2D, Dropout, Activation, BatchNormalization, multiply, add","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Dataset (downloading, extraction, organization)***","metadata":{}},{"cell_type":"code","source":"# Function to download the dataset\ndef download_file(url, dest):\n    response = requests.get(url, stream=True)\n    total_size = int(response.headers.get('content-length', 0))\n    block_size = 1024  # 1 Kibibyte\n    with open(dest, 'wb') as file, tqdm(\n        desc=\"Downloading zip file\",\n        total=total_size,\n        unit='iB',\n        unit_scale=True,\n        unit_divisor=1024,\n    ) as bar:\n        for data in response.iter_content(block_size):\n            bar.update(len(data))\n            file.write(data)\n\n# Function to extract the zip file\ndef extract_zip_with_progress(zip_path, extract_to):\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        total_files = len(zip_ref.infolist())\n        with tqdm(total=total_files, desc=\"Extracting zip file\") as pbar:\n            for file in zip_ref.infolist():\n                zip_ref.extract(file, extract_to)\n                pbar.update(1)\n\n# Function to organize, resize, and sorte images\ndef resize_and_save(src_dir, dst_dir_a, dst_dir_b, size=(512, 512)):\n    files = sorted(os.listdir(src_dir))\n    half_index = len(files) // 2  # Calculate the split index\n    with tqdm(total=len(files), desc=f\"Resizing (624*624 -> 512*512) and saving files from {src_dir}\") as pbar:\n        for i, filename in enumerate(files):\n            if filename.endswith('.png'):\n                img_path = os.path.join(src_dir, filename)\n                img = Image.open(img_path)\n                new_img = img.resize(size)\n                if i < half_index:\n                    new_img.save(os.path.join(dst_dir_a, filename))\n                else:\n                    new_img.save(os.path.join(dst_dir_b, filename))\n                pbar.update(1)\n\n# \"The dataset is available from Liang et al. (2021) at the following link: https://doi.org/10.5281/zenodo.4722095.\"\n# or\n# \"The dataset is available from Liang et al. (2022) at the following link: https://doi.org/10.1016/j.cageo.2022.105217\"\n\n# Download the zip file\nurl = \"https://zenodo.org/records/4722095/files/Bent_data_624.zip?download=1\"\nzip_path = \"/kaggle/working/Bent_data_624.zip\"\ndownload_file(url, zip_path)\n\n# Extract the zip file\nextract_zip_with_progress(zip_path, \"/kaggle/working/\")\n\n# Remove the zip file\nos.remove(zip_path)\nprint(\"Zip file removed\")\n\n# Define source and destination paths\nsource_dir = \"/kaggle/working/Bent_data_624\"\nsample_c_inputs = \"/kaggle/working/sample_c/inputs\"\nsample_c_labels = \"/kaggle/working/sample_c/labels\"\nsample_a_inputs = \"/kaggle/working/sample_a/inputs\"\nsample_a_labels = \"/kaggle/working/sample_a/labels\"\nsample_b_inputs = \"/kaggle/working/sample_b/inputs\"\nsample_b_labels = \"/kaggle/working/sample_b/labels\"\n\n# Create destination directories\nos.makedirs(sample_c_inputs, exist_ok=True)\nos.makedirs(sample_c_labels, exist_ok=True)\nos.makedirs(sample_a_inputs, exist_ok=True)\nos.makedirs(sample_a_labels, exist_ok=True)\nos.makedirs(sample_b_inputs, exist_ok=True)\nos.makedirs(sample_b_labels, exist_ok=True)\n\n# Resize and move files\nresize_and_save(os.path.join(source_dir, \"image_test\"), sample_c_inputs, sample_c_inputs)\nresize_and_save(os.path.join(source_dir, \"mask_test\"), sample_c_labels, sample_c_labels)\nresize_and_save(os.path.join(source_dir, \"image_train_valid_XYZ\"), sample_a_inputs, sample_b_inputs)\nresize_and_save(os.path.join(source_dir, \"mask_train_valid_XYZ\"), sample_a_labels, sample_b_labels)\n\n# Delete the old folder\nshutil.rmtree(source_dir)\nprint(\"Old folder deleted\")\n\nprint(\"All files have been organized, resized, and sorted\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ***Train & validation data prepration***","metadata":{}},{"cell_type":"markdown","source":"### ***Sample A, 20 uCT slices***","metadata":{}},{"cell_type":"markdown","source":"#### ***Inputs***","metadata":{}},{"cell_type":"code","source":"# The path of input images of sample A\nfolder_path = \"/kaggle/working/sample_a/inputs/\"\n\n# Listing all the image files in the folder and sorting them by name\nfiles = os.listdir(folder_path)\nimage_files = sorted([f for f in files if f.endswith(\".png\")])\n\n# Determining the step size to use for selecting images (100 steps in this case, including the first & last ones)\nstep = int(len(image_files) / (len(image_files) * 0.01) + 1)\n\n# Selecting the images\nselected_images = []\nfor i in range(0, len(image_files), step):\n    selected_images.append(image_files[i])\nselected_images = list(set(selected_images))\nselected_images.sort()\n\n# The first and last images must be included in the selection\nif image_files[0] not in selected_images:\n    selected_images.insert(0, image_files[0])\nif image_files[-1] not in selected_images:\n    selected_images.append(image_files[-1])\n\n# Move the selected images to a new folder\nnew_folder_path = \"/kaggle/working/train_val_inputs/100step/\"\nif not os.path.exists(new_folder_path):\n    os.makedirs(new_folder_path)\nfor image_file in selected_images:\n    shutil.copy(os.path.join(folder_path, image_file), new_folder_path)\n\nprint(f\"Selected and copied {len(selected_images)} images to {new_folder_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ***Labels***","metadata":{}},{"cell_type":"code","source":"# The path of label images of sample A\nfolder_path = \"/kaggle/working/sample_a/labels/\"\n\n# Listing all the image files in the folder and sorting them by name\nfiles = os.listdir(folder_path)\nimage_files = sorted([f for f in files if f.endswith(\".png\")])\n\n# Determining the step size to use for selecting images (100 steps in this case, including the first & last ones)\nstep = int(len(image_files) / (len(image_files) * 0.01) + 1)\n\n# Selecting the images\nselected_images = []\nfor i in range(0, len(image_files), step):\n    selected_images.append(image_files[i])\nselected_images = list(set(selected_images))\nselected_images.sort()\n\n# The first and last images must be included in the selection\nif image_files[0] not in selected_images:\n    selected_images.insert(0, image_files[0])\nif image_files[-1] not in selected_images:\n    selected_images.append(image_files[-1])\n\n# Move the selected images to a new folder\nnew_folder_path = \"/kaggle/working/train_val_labels/100step/\"\nif not os.path.exists(new_folder_path):\n    os.makedirs(new_folder_path)\nfor image_file in selected_images:\n    shutil.copy(os.path.join(folder_path, image_file), new_folder_path)\n\nprint(f\"Selected and copied {len(selected_images)} images to {new_folder_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ***Sample B, 20 uCT slices***","metadata":{}},{"cell_type":"markdown","source":"#### ***Inputs***","metadata":{}},{"cell_type":"code","source":"# The path of input images of sample B\nfolder_path = \"/kaggle/working/sample_b/inputs/\"\n\n# Listing all the image files in the folder and sorting them by name\nfiles = os.listdir(folder_path)\nimage_files = sorted([f for f in files if f.endswith(\".png\")])\n\n# Determining the step size to use for selecting images (100 steps in this case, including the first & last ones)\nstep = int(len(image_files) / (len(image_files) * 0.01) + 1)\n\n# Selecting the images\nselected_images = []\nfor i in range(0, len(image_files), step):\n    selected_images.append(image_files[i])\nselected_images = list(set(selected_images))\nselected_images.sort()\n\n# The first and last images must be included in the selection\nif image_files[0] not in selected_images:\n    selected_images.insert(0, image_files[0])\nif image_files[-1] not in selected_images:\n    selected_images.append(image_files[-1])\n\n# Move the selected images to a new folder\nnew_folder_path = \"/kaggle/working/train_val_inputs/100step/\"\nif not os.path.exists(new_folder_path):\n    os.makedirs(new_folder_path)\nfor image_file in selected_images:\n    shutil.copy(os.path.join(folder_path, image_file), new_folder_path)\n\nprint(f\"Selected and copied {len(selected_images)} images to {new_folder_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ***Labels***","metadata":{}},{"cell_type":"code","source":"# The path of input images of sample B\nfolder_path = \"/kaggle/working/sample_b/labels/\"\n\n# Listing all the image files in the folder and sorting them by name\nfiles = os.listdir(folder_path)\nimage_files = sorted([f for f in files if f.endswith(\".png\")])\n\n# Determining the step size to use for selecting images (100 steps in this case, including the first & last ones)\nstep = int(len(image_files) / (len(image_files) * 0.01) + 1)\n\n# Selecting the images\nselected_images = []\nfor i in range(0, len(image_files), step):\n    selected_images.append(image_files[i])\nselected_images = list(set(selected_images))\nselected_images.sort()\n\n# The first and last images must be included in the selection\nif image_files[0] not in selected_images:\n    selected_images.insert(0, image_files[0])\nif image_files[-1] not in selected_images:\n    selected_images.append(image_files[-1])\n\n# Move the selected images to a new folder\nnew_folder_path = \"/kaggle/working/train_val_labels/100step/\"\nif not os.path.exists(new_folder_path):\n    os.makedirs(new_folder_path)\nfor image_file in selected_images:\n    shutil.copy(os.path.join(folder_path, image_file), new_folder_path)\n\nprint(f\"Selected and copied {len(selected_images)} images to {new_folder_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ***2in1 (input_label)***","metadata":{}},{"cell_type":"code","source":"# Path to input and label folders\ninput_folder = \"/kaggle/working/train_val_inputs/100step/\"\nlabel_folder = \"/kaggle/working/train_val_labels/100step/\"\n\n# Create a folder for the combined images\ncom_folder = \"/kaggle/working/train_val_data/100step/\"\nos.makedirs(com_folder, exist_ok=True)\n\n# Get sorted lists of files in each folder\ninput_files = sorted([f for f in os.listdir(input_folder) if f.endswith('.png')])\nlabel_files = sorted([f for f in os.listdir(label_folder) if f.endswith('.png')])\n\n# Verify that both folders contain the same number of files\nif len(input_files) != len(label_files):\n    raise ValueError(\"The number of input files and label files do not match.\")\n\n# Loop through the 40\nfor i in range(min(40, len(input_files))):\n    input_image_path = os.path.join(input_folder, input_files[i])\n    label_image_path = os.path.join(label_folder, label_files[i])\n\n    # Read the images in grayscale\n    input_image = cv2.imread(input_image_path, cv2.IMREAD_GRAYSCALE)\n    label_image = cv2.imread(label_image_path, cv2.IMREAD_GRAYSCALE)\n\n    # Check if both images are successfully loaded\n    if input_image is None or label_image is None:\n        print(f\"Error loading images at index {i}: {input_image_path} or {label_image_path}\")\n        continue\n\n    # Combine input and label images horizontally\n    combined_image = cv2.hconcat([input_image, label_image])\n\n    # Save combined image to a folder\n    combined_image_path = os.path.join(com_folder, f\"Bentheimer_{i:04d}.png\")\n    cv2.imwrite(combined_image_path, combined_image)\n\nshutil.rmtree(\"/kaggle/working/train_val_inputs\")\nshutil.rmtree(\"/kaggle/working/train_val_labels\")\nprint(f\"Processed {min(40, len(input_files))} images.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Downloading the train & validation data (If needed)\ndef zip_directory(folder_path, output_path):\n    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(folder_path):\n            for file in files:\n                # Create a proper path relative to the folder being zipped\n                file_path = os.path.join(root, file)\n                zipf.write(file_path, os.path.relpath(file_path, os.path.join(folder_path, '..')))\n\n# Define the path to the folder you want to zip\nfolder_path = \"/kaggle/working/train_val_data/100step\"\n\n# Define the output path for the zip file\noutput_zip_path = \"/kaggle/working/train_val_data.zip\"\n\n# Zip the directory\nzip_directory(folder_path, output_zip_path)\n\nprint(f\"Created zip file at {output_zip_path}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ***Train & validation data preprocessing, augmentation and generators***","metadata":{}},{"cell_type":"code","source":"# Get the list of file paths for input images\nfile_paths = tf.data.Dataset.list_files(\"/kaggle/working/train_val_data/100step/*.png\")\n\n# Convert the dataset to a list\nfile_paths = list(file_paths.as_numpy_iterator())\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices(file_paths)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining load function\ndef load(image_file):\n  # Read and decode an image file to a uint8 tensor\n  image = tf.io.read_file(image_file)\n  image = tf.io.decode_png(image)\n\n  # Split each image tensor into two tensors\n  w = tf.shape(image)[1]\n  w = w // 2\n  input = image[:, :w, :]\n  label = image[:, w:, :]\n\n  # Convert images to float16 tensors\n  input = tf.cast(input, tf.float32)\n  label = tf.cast(label, tf.float32)\n\n  return input, label\n\n# Defining normalize function\ndef normalize(input, label):\n    \n  # Normalize the input image by scaling its pixel values to the range [-1, 1]\n  input = (input / 127.5) - 1\n  \n  # Normalize the label image by scaling its pixel values to the range [-1, 1]\n  label = (label - 1.5) * (2/3)\n    \n  # Return the normalized input and label images\n  return input, label\n\n# Defining resize function\ndef r_size(input, label, height, width):\n    \n  # Resize the input image to the desired height and width using nearest neighbor interpolation\n  input = tf.image.resize(input, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    \n  # Resize the label image to the desired height and width using nearest neighbor interpolation\n  label = tf.image.resize(label, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    \n  # Return the resized input and label images\n  return input, label\n\n# Each image is 512x512 in size\nIMG_WIDTH = 512\nIMG_HEIGHT = 512\n\n# Defining random_crop function\ndef random_crop(input, label):\n    \n  # Stack the input and label images along the first dimension\n  stacked_image = tf.stack([input, label], axis=0)\n    \n  # Randomly crop the stacked image to the desired size\n  cropped_image = tf.image.random_crop(stacked_image, size=[2, IMG_WIDTH, IMG_HEIGHT, 1])\n    \n  # Return the cropped input and label images\n  return cropped_image[0], cropped_image[1]\n\n# Defining random_shear function\ndef random_shear(input, label, shear_range):\n    shear_factor = tf.random.uniform([], minval=-shear_range, maxval=shear_range)\n    transformed_input = tfa.image.transform(input, [1.0, shear_factor, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], fill_mode='reflect')\n    transformed_label = tfa.image.transform(label, [1.0, shear_factor, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], fill_mode='reflect')\n    return transformed_input, transformed_label\n\n# Defining random_zoom function\ndef random_zoom(input, label, zoom_range):\n    # Random zoom factor between 1/zoom_range and zoom_range\n    zoom_factor = tf.random.uniform([], minval=1/zoom_range, maxval=zoom_range)\n    zoom_factor = tf.cast(zoom_factor, input.dtype)\n    \n    if zoom_factor < 1.0:\n        # Zoom out: Pad the images\n        pad_size_height = tf.cast((1.0 - zoom_factor) * tf.cast(tf.shape(input)[0], tf.float32) / 2, tf.int32)\n        pad_size_width = tf.cast((1.0 - zoom_factor) * tf.cast(tf.shape(input)[1], tf.float32) / 2, tf.int32)\n        padding = [[pad_size_height, pad_size_height], [pad_size_width, pad_size_width], [0, 0]]\n        \n        padded_input = tf.pad(input, padding, mode='REFLECT')\n        padded_label = tf.pad(label, padding, mode='REFLECT')\n        \n        resized_input = tf.image.resize(padded_input, tf.shape(input)[:2])\n        resized_label = tf.image.resize(padded_label, tf.shape(label)[:2])\n    else:\n        # Zoom in: Crop the images\n        central_fraction = 1.0 / zoom_factor\n        cropped_input = tf.image.central_crop(input, central_fraction)\n        cropped_label = tf.image.central_crop(label, central_fraction)\n        \n        resized_input = tf.image.resize(cropped_input, tf.shape(input)[:2])\n        resized_label = tf.image.resize(cropped_label, tf.shape(label)[:2])\n    \n    return resized_input, resized_label\n\n# Defining random_jitter function\n@tf.function()\ndef random_jitter(input, label):\n    \n    # Randomly resize the input and label images to (512+30)x(512+30)\n    if tf.random.uniform(()) > 0.5:\n        input, label = r_size(input, label, IMG_WIDTH+30, IMG_WIDTH+30)\n    # Randomly crop the input and label images to 512x512\n        input, label = random_crop(input, label)\n        \n    # Randomly flip the images left to right with 50% probability\n    if tf.random.uniform(()) > 0.5:\n        input = tf.image.flip_left_right(input)\n        label = tf.image.flip_left_right(label)\n    \n    # Randomly flip the images up & down with 50% probability\n    if tf.random.uniform(()) > 0.5:\n        input = tf.image.flip_up_down(input)\n        label = tf.image.flip_up_down(label)\n    \n    # Randomly rotate the images with 50% probability\n    if tf.random.uniform(()) > 0.5:\n        input = tfa.image.rotate(input, tf.constant(np.pi/6), 'nearest', fill_mode='reflect')\n        label = tfa.image.rotate(label, tf.constant(np.pi/6), 'nearest', fill_mode='reflect')\n        \n    # Randomly zoom the images with 50% probability\n    if tf.random.uniform(()) > 0.5:\n        input, label = random_zoom(input, label, 1.1) \n        \n    # Randomly shear the images with 50% probability\n    if tf.random.uniform(()) > 0.5:\n        input, label = random_shear(input, label, 0.1)\n        \n    # Return the randomly jittered input and label images\n    return input, label\n\n# Defining preparing function\ndef preparing(input, label):\n    \n  # Apply random jitter to the images\n  input, label = random_jitter(input, label)\n\n  # Return the preprocessed images\n  return input, label\n\n# Defining preparing training data function\ndef prep_train(image_file):\n  # Load the input and label images from the given image file\n  input, label = load(image_file)\n    \n  # Normalize the input and label images\n  input, label = normalize(input, label)\n    \n  # Preparing the  images\n  input, label = preparing(input, label)\n        \n  # Return the prepared images\n  return input, label\n\n# Defining preparing validation data function\ndef prep_val(image_file):\n  # Split the images from the given image file\n  input, label = load(image_file)\n    \n  # Normalize the images\n  input, label = normalize(input, label)\n    \n  # Return the prepared images\n  return input, label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining generators\ndef generator(file_paths, batch_size, split_ratio, validation=False):\n    while True:\n        random.shuffle(file_paths)\n\n        train_paths = file_paths[:int(split_ratio * len(file_paths))]\n        val_paths = file_paths[int(split_ratio * len(file_paths)):]\n\n        try:\n            train_dataset = tf.data.Dataset.from_tensor_slices(train_paths)\n        except tf.errors.InvalidArgumentError:\n            train_dataset = tf.data.Dataset.from_tensor_slices(train_paths)\n        train_dataset = train_dataset.map(prep_train)\n        train_dataset = train_dataset.batch(batch_size)\n\n        try:\n            val_dataset = tf.data.Dataset.from_tensor_slices(val_paths)\n        except tf.errors.InvalidArgumentError:\n            val_dataset = tf.data.Dataset.from_tensor_slices(val_paths)\n        val_dataset = val_dataset.map(prep_val)\n        val_dataset = val_dataset.batch(batch_size)\n\n        if not validation:\n            for input, label in train_dataset:\n                \n                # [0, 1]\n                input = (input + 1) / 2.0\n                input = np.array(input)\n                \n                # [0, 3]\n                label = (label * 3/2) + 1.5\n                label = np.round(np.clip(label, 0, 3))\n                \n                label = to_categorical(\n                    label, num_classes=4)[:, :, :, 0:]\n\n                yield input, label\n\n        else:\n            for input, label in val_dataset:\n                \n                # [0, 1]\n                input = (input + 1) / 2.0\n                input = np.array(input)\n                \n                # [0, 3]\n                label = (label * 3/2) + 1.5\n                label = np.round(np.clip(label, 0, 3))\n                \n                label = to_categorical(\n                    label, num_classes=4)[:, :, :, 0:]\n\n                yield input, label\n                \ntrain_generator = generator(file_paths, 8, 0.8, validation=False)\nval_generator = generator(file_paths, 8, 0.8, validation=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***U-Net***","metadata":{}},{"cell_type":"code","source":"# Optimizer\noptimizer = Adam(learning_rate=1e-3)\n\n# Define the convolutional block\ndef conv2d_block(input_tensor, n_filters, kernel_size=3, batchnorm=True):\n    \n    # first layer\n    x1 = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size),\n                kernel_initializer=\"he_uniform\",\n                dilation_rate=(1, 1), padding=\"same\", kernel_regularizer=regularizers.l2(1e-4), bias_regularizer=regularizers.l2(1e-4))(input_tensor)\n    if batchnorm:\n        x1 = BatchNormalization()(x1)\n        x1 = Activation(\"relu\")(x1)\n        \n    # second layer\n    x = Conv2D(filters=n_filters*1, kernel_size=(kernel_size, kernel_size),\n               kernel_initializer=\"he_uniform\",\n               dilation_rate=(1, 1), padding=\"same\", kernel_regularizer=regularizers.l2(1e-4), bias_regularizer=regularizers.l2(1e-4))(x1)\n    if batchnorm:\n        x = BatchNormalization()(x)  \n    x = Activation(\"relu\")(x)\n    return x\n\n# Define the model architecture\ndef model(input_img, n_filters=16, batchnorm=True):\n    \n    # Encoding Path\n    c1 = conv2d_block(input_img, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n    p1 = MaxPooling2D((2, 2)) (c1)\n \n    c2 = conv2d_block(p1, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n    p2 = MaxPooling2D((2, 2)) (c2)\n    \n    c3 = conv2d_block(p2, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n    p3 = MaxPooling2D((2, 2)) (c3)\n        \n    c4 = conv2d_block(p3, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n    \n    # Bottom\n    c5 = conv2d_block(p4, n_filters=n_filters*16, kernel_size=3, batchnorm=batchnorm)\n    \n    # Decoding Path\n    u6 = Conv2DTranspose(n_filters*8, (3, 3), strides=(2, 2), dilation_rate=(1, 1), padding='same') (c5)\n    u6 = concatenate([u6, c4])\n    c6 = conv2d_block(u6, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n\n    u7 = Conv2DTranspose(n_filters*4, (3, 3), strides=(2, 2), dilation_rate=(1, 1), padding='same') (c6)\n    u7 = concatenate([u7, c3])\n    c7 = conv2d_block(u7, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n\n    u8 = Conv2DTranspose(n_filters*2, (3, 3), strides=(2, 2), dilation_rate=(1, 1), padding='same') (c7)\n    u8 = concatenate([u8, c2])\n    c8 = conv2d_block(u8, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n\n    u9 = Conv2DTranspose(n_filters*1, (3, 3), strides=(2, 2), dilation_rate=(1, 1), padding='same') (c8)\n    u9 = concatenate([u9, c1], axis=3)\n    c9 = conv2d_block(u9, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n    \n    # Classifier part\n    outputs = Conv2D(4, (1, 1), activation='softmax') (c9) # softmax or sigmoid?\n    \n    # Define the model\n    model = Model(inputs=[input_img], outputs=[outputs])\n    return model\n\n# Define the input shape\ninput_img = Input((512, 512, 1))\n\n# Get the model\ndef get_model():\n    return model(input_img, n_filters=16, batchnorm=True)\n\nmodel = get_model()\n\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Train & validation***","metadata":{}},{"cell_type":"code","source":"# Instantiate custom callbacks\n\n# 1 Define a callback to reduce learning rate for a specific situation\nrlr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, min_lr=1e-5, verbose=2)\n\n# 2 Define a callback to stop training process for a specific situation\nes = EarlyStopping(monitor='val_loss', mode='min', min_delta=1e-4, patience=8)\n\n# 3 Define a callback to save history logs\ncsvl = CSVLogger(\"./history_log.csv\", append=True)\n\n# 4 Define a callback to save the best weights\ncheckpoint_dir = \"./weights/\"\nif not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)\n\ncheckpoint_filepath = os.path.join(checkpoint_dir, \"best_weights.hdf5\")\nchp = ModelCheckpoint(checkpoint_filepath, monitor='loss', verbose=1, save_weights_only=True, save_best_only=True, mode='auto')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training & validation procedure\nnb_epoch = 1000\nbatch_size = 1\n\n# Train the model\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=1.6*len(file_paths) // batch_size,\n    \n    validation_data=val_generator,\n    validation_steps=0.4*len(file_paths) // batch_size,\n    \n    epochs=nb_epoch, verbose=1,\n        \n    callbacks=[ # Setting callbacks\n        csvl,\n        rlr,\n        chp,\n        es\n    ]\n    \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the history of training and validation loss and accuracy\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']\n\n# Plot the loss\nplt.plot(train_loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend()\nplt.title('Loss')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Evaluation & blind test***","metadata":{}},{"cell_type":"code","source":"# Due to RAM limits\nimport gc\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ***Load the best weight***","metadata":{}},{"cell_type":"code","source":"model.load_weights('/kaggle/working/weights/best_weights.hdf5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ***Preparing test samples (a, b, c)***","metadata":{}},{"cell_type":"code","source":"# Sample a (modify for a, b or c)\ninput_dir = '/kaggle/working/sample_c/inputs/'\nmask_dir = '/kaggle/working/sample_c/labels/'\n\ninput_images = io.imread_collection(os.path.join(input_dir, '*.png'))\nmask_images = io.imread_collection(os.path.join(mask_dir, '*.png'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inputs\nx_test = []\nfor img in input_images:\n    img_norm = img.astype('float16') / 255.0\n    x_test.append(img_norm)\nx_test = np.array(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Labels\ny_test = []\nfor img in mask_images:\n    img_onehot = to_categorical(img, num_classes=4)[:, :, 0:] # The background class (0) is excluded\n    y_test.append(img_onehot)\ny_test = np.array(y_test)\ny_test = y_test.astype('float16')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving path\ntest_dir = \"/kaggle/working/test_data/\"\nif not os.path.exists(test_dir):\n    os.makedirs(test_dir)\n# Save the arrays\nnp.save(test_dir + '/x_test.npy', x_test)\nnp.save(test_dir + '/y_test.npy', y_test)\n\ndel x_test, y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test = np.load(test_dir + '/x_test.npy')\n\nbatch_size = 100\ny_pred = []\nfor i in range(0, len(x_test), batch_size):\n    batch_x_test = x_test[i:i+batch_size]\n    batch_y_pred = model.predict(batch_x_test)\n    y_pred.append(batch_y_pred)\n    print(i)\ny_pred = np.concatenate(y_pred, axis=0)\ny_pred = y_pred.astype('float16')\n\ndel x_test","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ***Evaluation metrics***","metadata":{}},{"cell_type":"markdown","source":"### ***Pore***","metadata":{}},{"cell_type":"code","source":"# Compute confusion matrix for 1st class\ny_test = np.load(test_dir + '/y_test.npy')\ntrue = np.argmax(y_test, axis=-1) == 0\ndel y_test\n\npred = np.argmax(y_pred, axis=-1) == 0\n\ncm = confusion_matrix(true.ravel(), pred.ravel())\n\n# Normalize confusion matrix by true class frequencies\ncm_norm = cm / cm.sum(axis=1)[:, np.newaxis]\n\n# Plot confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_norm)\ndisp.plot(cmap = 'Blues')\ndisp.ax_.set_title('Confusion matrix: Macro pore space')\nplt.show()\n\naccuracy = (cm_norm[0, 0] + cm_norm[1, 1]) / np.sum(cm_norm)\n\nPrecision = cm_norm[1][1] / (cm_norm[1][1] + cm_norm[1][0])\nRecall = cm_norm[1][1] / (cm_norm[1][1] + cm_norm[0][1])\nF1 = 2 * (Precision * Recall) / (Precision + Recall)\niou = cm_norm[1, 1] / (cm_norm[1, 1] + cm_norm[0, 1] + cm_norm[1, 0])\n\nprint('Accuracy:', accuracy)\nprint('Precision:', Precision)\nprint('Recall:', Recall)\nprint('F1-Score:', F1)\nprint('Intersection over Union (IoU):', iou)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ***Quartz***","metadata":{}},{"cell_type":"code","source":"# Compute confusion matrix for 2nd class\ny_test = np.load(test_dir + '/y_test.npy')\ntrue = np.argmax(y_test, axis=-1) == 1\ndel y_test\n\npred = np.argmax(y_pred, axis=-1) == 1\n\ncm = confusion_matrix(true.ravel(), pred.ravel())\n\n# Normalize confusion matrix by true class frequencies\ncm_norm = cm / cm.sum(axis=1)[:, np.newaxis]\n\n# Plot confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_norm)\ndisp.plot(cmap = 'Blues')\ndisp.ax_.set_title('Confusion matrix: Quartz')\nplt.show()\n\naccuracy = (cm_norm[0, 0] + cm_norm[1, 1]) / np.sum(cm_norm)\n\nPrecision = cm_norm[1][1] / (cm_norm[1][1] + cm_norm[1][0])\nRecall = cm_norm[1][1] / (cm_norm[1][1] + cm_norm[0][1])\nF1 = 2 * (Precision * Recall) / (Precision + Recall)\niou = cm_norm[1, 1] / (cm_norm[1, 1] + cm_norm[0, 1] + cm_norm[1, 0])\n\nprint('Accuracy:', accuracy)\nprint('Precision:', Precision)\nprint('Recall:', Recall)\nprint('F1-Score:', F1)\nprint('Intersection over Union (IoU):', iou)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ***Clay***","metadata":{}},{"cell_type":"code","source":"# Compute confusion matrix for 3rd class\ny_test = np.load(test_dir + '/y_test.npy')\ntrue = np.argmax(y_test, axis=-1) == 2\ndel y_test\n\npred = np.argmax(y_pred, axis=-1) == 2\n\ncm = confusion_matrix(true.ravel(), pred.ravel())\n\n# Normalize confusion matrix by true class frequencies\ncm_norm = cm / cm.sum(axis=1)[:, np.newaxis]\n\n# Plot confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_norm)\ndisp.plot(cmap = 'Blues')\ndisp.ax_.set_title('Confusion matrix: Clay region')\nplt.show()\n\naccuracy = (cm_norm[0, 0] + cm_norm[1, 1]) / np.sum(cm_norm)\n\nPrecision = cm_norm[1][1] / (cm_norm[1][1] + cm_norm[1][0])\nRecall = cm_norm[1][1] / (cm_norm[1][1] + cm_norm[0][1])\nF1 = 2 * (Precision * Recall) / (Precision + Recall)\niou = cm_norm[1, 1] / (cm_norm[1, 1] + cm_norm[0, 1] + cm_norm[1, 0])\n\nprint('Accuracy:', accuracy)\nprint('Precision:', Precision)\nprint('Recall:', Recall)\nprint('F1-Score:', F1)\nprint('Intersection over Union (IoU):', iou)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ***Feldspar***","metadata":{}},{"cell_type":"code","source":"# Compute confusion matrix for 4th class\ny_test = np.load(test_dir + '/y_test.npy')\ntrue = np.argmax(y_test, axis=-1) == 3\ndel y_test\n\npred = np.argmax(y_pred, axis=-1) == 3\n\ncm = confusion_matrix(true.ravel(), pred.ravel())\n\n# Normalize confusion matrix by true class frequencies\ncm_norm = cm / cm.sum(axis=1)[:, np.newaxis]\n\n# Plot confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_norm)\ndisp.plot(cmap = 'Blues')\ndisp.ax_.set_title('Confusion matrix: Feldspar')\nplt.show()\n\naccuracy = (cm_norm[0, 0] + cm_norm[1, 1]) / np.sum(cm_norm)\n\nPrecision = cm_norm[1][1] / (cm_norm[1][1] + cm_norm[1][0])\nRecall = cm_norm[1][1] / (cm_norm[1][1] + cm_norm[0][1])\nF1 = 2 * (Precision * Recall) / (Precision + Recall)\niou = cm_norm[1, 1] / (cm_norm[1, 1] + cm_norm[0, 1] + cm_norm[1, 0])\n\nprint('Accuracy:', accuracy)\nprint('Precision:', Precision)\nprint('Recall:', Recall)\nprint('F1-Score:', F1)\nprint('Intersection over Union (IoU):', iou)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"slide_number = 0\n\n# Load inputs and labels\nx_test = np.load('/kaggle/working/test_data/x_test.npy')[slide_number]\ny_test = np.argmax(np.load('/kaggle/working/test_data/y_test.npy')[slide_number], axis=-1)\ny_pred0 = np.argmax(y_pred[slide_number], axis=-1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to plot\ndef plot_input_and_label(x_test, y_test, cmap='gray'):\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    axes[0].imshow(x_test, cmap=cmap)\n    axes[0].set_title(\"Input\")\n\n    axes[1].imshow(y_test, cmap=cmap)\n    axes[1].set_title(\"Label\")\n\n    # Legend for the Label image\n    legend_labels = [Patch(facecolor='black', label='Pore'),\n                    Patch(facecolor='gray', label='Quartz'),\n                    Patch(facecolor='darkgray', label='Clay'),\n                    Patch(facecolor='white', label='Feldspar')]\n    axes[1].legend(handles=legend_labels, loc='lower right', fontsize=10)\n    \n    # Add rectangle to the \"Label\" plot i.e., axes[1]\n    rect = Rectangle((2, y_test.shape[0] - 258), 257, 256, linewidth=2, edgecolor='r', facecolor='none')\n    axes[1].add_patch(rect)\n    \n    plt.tight_layout()\n    plt.show()\n    \nplot_input_and_label(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_predictions_and_errors(x_test, y_test, y_pred, index=0):\n    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n    \n    # Normalize\n    norm = Normalize(vmin=0, vmax=1)\n    \n    # Plot Input\n    axes[0].imshow(x_test[256:512, :256], cmap='gray')\n    axes[0].set_title(\"Input\")\n    \n    # Plot Label\n    axes[1].imshow(y_test[256:512, :256], cmap='gray')\n    axes[1].set_title(\"True Label\")\n    \n    # Plot Prediction\n    axes[2].imshow(y_pred0[256:512, :256], cmap='gray')\n    axes[2].set_title(\"Prediction\")\n    \n    # Plot Error Map\n    error_map = np.abs(y_pred0 - y_test)\n    error_map = np.where(error_map > 0, 1, 0)\n    axes[3].imshow(error_map[256:512, :256], cmap='gray')\n    axes[3].set_title(\"Error Map\")\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_predictions_and_errors(x_test, y_test, y_pred, index=0)","metadata":{},"execution_count":null,"outputs":[]}]}