{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ***Packages***","metadata":{}},{"cell_type":"code","source":"# To ensure compatiblity of TensorFlow Addons with TensorFlow\n!pip install tensorflow-addons[tensorflow]","metadata":{"execution":{"iopub.execute_input":"2024-06-21T04:11:24.879713Z","iopub.status.busy":"2024-06-21T04:11:24.878876Z","iopub.status.idle":"2024-06-21T04:11:41.426818Z","shell.execute_reply":"2024-06-21T04:11:41.425756Z","shell.execute_reply.started":"2024-06-21T04:11:24.879682Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport time\nimport glob\nimport scipy\nimport random\nimport shutil\nimport pathlib\nimport zipfile\nimport requests\nimport datetime\nimport tifffile\nimport platform\nimport ipykernel\nimport itertools\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport seaborn as sns\nfrom skimage import io\nimport tensorflow as tf\nfrom keras.models import *\nfrom keras.layers import *\nfrom IPython import display\nfrom tensorflow import keras\nfrom skimage import io, color\nfrom keras.models import Model\nfrom keras import backend as K\nimport matplotlib.pyplot as plt\nimport tensorflow_addons as tfa\nfrom matplotlib.patches import Patch\nfrom skimage.transform import resize\nfrom skimage.io import imread, imsave\nfrom keras.utils import to_categorical\nfrom sklearn.utils import class_weight\nfrom matplotlib.colors import Normalize\nfrom skimage.util import view_as_windows\nfrom keras.metrics import Precision, Recall\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.models import load_model\nfrom matplotlib.patches import Rectangle, Patch\nfrom scipy.ndimage import distance_transform_edt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import LambdaCallback\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras import layers, models, regularizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint\nfrom sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\nfrom tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, UpSampling2D, Dropout, Activation, BatchNormalization, multiply, add","metadata":{"execution":{"iopub.execute_input":"2024-06-21T05:57:09.558455Z","iopub.status.busy":"2024-06-21T05:57:09.558034Z","iopub.status.idle":"2024-06-21T05:57:09.573468Z","shell.execute_reply":"2024-06-21T05:57:09.572297Z","shell.execute_reply.started":"2024-06-21T05:57:09.558411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### ***Ignore the errors***","metadata":{}},{"cell_type":"markdown","source":"# ***Dataset (downloading, extraction, organization)***","metadata":{}},{"cell_type":"code","source":"# Function to download the dataset\ndef download_file(url, dest):\n    response = requests.get(url, stream=True)\n    total_size = int(response.headers.get('content-length', 0))\n    block_size = 1024  # 1 Kibibyte\n    with open(dest, 'wb') as file, tqdm(\n        desc=\"Downloading zip file\",\n        total=total_size,\n        unit='iB',\n        unit_scale=True,\n        unit_divisor=1024,\n    ) as bar:\n        for data in response.iter_content(block_size):\n            bar.update(len(data))\n            file.write(data)\n\n# Function to extract the zip file\ndef extract_zip_with_progress(zip_path, extract_to):\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        total_files = len(zip_ref.infolist())\n        with tqdm(total=total_files, desc=\"Extracting zip file\") as pbar:\n            for file in zip_ref.infolist():\n                zip_ref.extract(file, extract_to)\n                pbar.update(1)\n\n# Function to organize, resize, and sorte images\ndef resize_and_save(src_dir, dst_dir_a, dst_dir_b, size=(512, 512)):\n    files = sorted(os.listdir(src_dir))\n    half_index = len(files) // 2  # Calculate the split index\n    with tqdm(total=len(files), desc=f\"Resizing (624*624 -> 512*512) and saving files from {src_dir}\") as pbar:\n        for i, filename in enumerate(files):\n            if filename.endswith('.png'):\n                img_path = os.path.join(src_dir, filename)\n                img = Image.open(img_path)\n                new_img = img.resize(size)\n                if i < half_index:\n                    new_img.save(os.path.join(dst_dir_a, filename))\n                else:\n                    new_img.save(os.path.join(dst_dir_b, filename))\n                pbar.update(1)\n\n# \"The dataset is available from Liang et al. (2021) at the following link: https://doi.org/10.5281/zenodo.4722095.\"\n# or\n# \"The dataset is available from Liang et al. (2022) at the following link: https://doi.org/10.1016/j.cageo.2022.105217\"\n\n# Download the zip file\nurl = \"https://zenodo.org/records/4722095/files/Bent_data_624.zip?download=1\"\nzip_path = \"/kaggle/working/Bent_data_624.zip\"\ndownload_file(url, zip_path)\n\n# Extract the zip file\nextract_zip_with_progress(zip_path, \"/kaggle/working/\")\n\n# Remove the zip file\nos.remove(zip_path)\nprint(\"Zip file removed\")\n\n# Define source and destination paths\nsource_dir = \"/kaggle/working/Bent_data_624\"\nsample_c_inputs = \"/kaggle/working/sample_c/inputs\"\nsample_c_labels = \"/kaggle/working/sample_c/labels\"\nsample_a_inputs = \"/kaggle/working/sample_a/inputs\"\nsample_a_labels = \"/kaggle/working/sample_a/labels\"\nsample_b_inputs = \"/kaggle/working/sample_b/inputs\"\nsample_b_labels = \"/kaggle/working/sample_b/labels\"\n\n# Create destination directories\nos.makedirs(sample_c_inputs, exist_ok=True)\nos.makedirs(sample_c_labels, exist_ok=True)\nos.makedirs(sample_a_inputs, exist_ok=True)\nos.makedirs(sample_a_labels, exist_ok=True)\nos.makedirs(sample_b_inputs, exist_ok=True)\nos.makedirs(sample_b_labels, exist_ok=True)\n\n# Resize and move files\nresize_and_save(os.path.join(source_dir, \"image_test\"), sample_c_inputs, sample_c_inputs)\nresize_and_save(os.path.join(source_dir, \"mask_test\"), sample_c_labels, sample_c_labels)\nresize_and_save(os.path.join(source_dir, \"image_train_valid_XYZ\"), sample_a_inputs, sample_b_inputs)\nresize_and_save(os.path.join(source_dir, \"mask_train_valid_XYZ\"), sample_a_labels, sample_b_labels)\n\n# Delete the old folder\nshutil.rmtree(source_dir)\nprint(\"Old folder deleted\")\n\nprint(\"All files have been organized, resized, and sorted\")","metadata":{"execution":{"iopub.execute_input":"2024-06-21T04:13:21.576810Z","iopub.status.busy":"2024-06-21T04:13:21.575591Z","iopub.status.idle":"2024-06-21T04:19:07.740516Z","shell.execute_reply":"2024-06-21T04:19:07.739293Z","shell.execute_reply.started":"2024-06-21T04:13:21.576777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ***Train & validation data prepration***","metadata":{}},{"cell_type":"markdown","source":"### ***Sample A, 20 uCT slices***","metadata":{}},{"cell_type":"markdown","source":"#### ***Inputs***","metadata":{}},{"cell_type":"code","source":"# The path of input images of sample A\nfolder_path = \"/kaggle/working/sample_a/inputs/\"\n\n# Listing all the image files in the folder and sorting them by name\nfiles = os.listdir(folder_path)\nimage_files = sorted([f for f in files if f.endswith(\".png\")])\n\n# Determining the step size to use for selecting images (100 steps in this case, including the first & last ones)\nstep = int(len(image_files) / (len(image_files) * 0.01) + 1)\n\n# Selecting the images\nselected_images = []\nfor i in range(0, len(image_files), step):\n    selected_images.append(image_files[i])\nselected_images = list(set(selected_images))\nselected_images.sort()\n\n# The first and last images must be included in the selection\nif image_files[0] not in selected_images:\n    selected_images.insert(0, image_files[0])\nif image_files[-1] not in selected_images:\n    selected_images.append(image_files[-1])\n\n# Move the selected images to a new folder\nnew_folder_path = \"/kaggle/working/train_val_inputs/100step/\"\nif not os.path.exists(new_folder_path):\n    os.makedirs(new_folder_path)\nfor image_file in selected_images:\n    shutil.copy(os.path.join(folder_path, image_file), new_folder_path)\n\nprint(f\"Selected and copied {len(selected_images)} images to {new_folder_path}\")","metadata":{"execution":{"iopub.execute_input":"2024-06-21T04:20:30.426151Z","iopub.status.busy":"2024-06-21T04:20:30.425305Z","iopub.status.idle":"2024-06-21T04:20:30.443911Z","shell.execute_reply":"2024-06-21T04:20:30.442827Z","shell.execute_reply.started":"2024-06-21T04:20:30.426118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ***Labels***","metadata":{}},{"cell_type":"code","source":"# The path of label images of sample A\nfolder_path = \"/kaggle/working/sample_a/labels/\"\n\n# Listing all the image files in the folder and sorting them by name\nfiles = os.listdir(folder_path)\nimage_files = sorted([f for f in files if f.endswith(\".png\")])\n\n# Determining the step size to use for selecting images (100 steps in this case, including the first & last ones)\nstep = int(len(image_files) / (len(image_files) * 0.01) + 1)\n\n# Selecting the images\nselected_images = []\nfor i in range(0, len(image_files), step):\n    selected_images.append(image_files[i])\nselected_images = list(set(selected_images))\nselected_images.sort()\n\n# The first and last images must be included in the selection\nif image_files[0] not in selected_images:\n    selected_images.insert(0, image_files[0])\nif image_files[-1] not in selected_images:\n    selected_images.append(image_files[-1])\n\n# Move the selected images to a new folder\nnew_folder_path = \"/kaggle/working/train_val_labels/100step/\"\nif not os.path.exists(new_folder_path):\n    os.makedirs(new_folder_path)\nfor image_file in selected_images:\n    shutil.copy(os.path.join(folder_path, image_file), new_folder_path)\n\nprint(f\"Selected and copied {len(selected_images)} images to {new_folder_path}\")","metadata":{"execution":{"iopub.execute_input":"2024-06-21T04:20:31.857236Z","iopub.status.busy":"2024-06-21T04:20:31.856326Z","iopub.status.idle":"2024-06-21T04:20:31.874081Z","shell.execute_reply":"2024-06-21T04:20:31.872995Z","shell.execute_reply.started":"2024-06-21T04:20:31.857191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ***Sample B, 20 uCT slices***","metadata":{}},{"cell_type":"markdown","source":"#### ***Inputs***","metadata":{}},{"cell_type":"code","source":"# The path of input images of sample B\nfolder_path = \"/kaggle/working/sample_b/inputs/\"\n\n# Listing all the image files in the folder and sorting them by name\nfiles = os.listdir(folder_path)\nimage_files = sorted([f for f in files if f.endswith(\".png\")])\n\n# Determining the step size to use for selecting images (100 steps in this case, including the first & last ones)\nstep = int(len(image_files) / (len(image_files) * 0.01) + 1)\n\n# Selecting the images\nselected_images = []\nfor i in range(0, len(image_files), step):\n    selected_images.append(image_files[i])\nselected_images = list(set(selected_images))\nselected_images.sort()\n\n# The first and last images must be included in the selection\nif image_files[0] not in selected_images:\n    selected_images.insert(0, image_files[0])\nif image_files[-1] not in selected_images:\n    selected_images.append(image_files[-1])\n\n# Move the selected images to a new folder\nnew_folder_path = \"/kaggle/working/train_val_inputs/100step/\"\nif not os.path.exists(new_folder_path):\n    os.makedirs(new_folder_path)\nfor image_file in selected_images:\n    shutil.copy(os.path.join(folder_path, image_file), new_folder_path)\n\nprint(f\"Selected and copied {len(selected_images)} images to {new_folder_path}\")","metadata":{"execution":{"iopub.execute_input":"2024-06-21T04:20:33.735614Z","iopub.status.busy":"2024-06-21T04:20:33.734756Z","iopub.status.idle":"2024-06-21T04:20:33.756998Z","shell.execute_reply":"2024-06-21T04:20:33.755917Z","shell.execute_reply.started":"2024-06-21T04:20:33.735574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ***Labels***","metadata":{}},{"cell_type":"code","source":"# The path of input images of sample B\nfolder_path = \"/kaggle/working/sample_b/labels/\"\n\n# Listing all the image files in the folder and sorting them by name\nfiles = os.listdir(folder_path)\nimage_files = sorted([f for f in files if f.endswith(\".png\")])\n\n# Determining the step size to use for selecting images (100 steps in this case, including the first & last ones)\nstep = int(len(image_files) / (len(image_files) * 0.01) + 1)\n\n# Selecting the images\nselected_images = []\nfor i in range(0, len(image_files), step):\n    selected_images.append(image_files[i])\nselected_images = list(set(selected_images))\nselected_images.sort()\n\n# The first and last images must be included in the selection\nif image_files[0] not in selected_images:\n    selected_images.insert(0, image_files[0])\nif image_files[-1] not in selected_images:\n    selected_images.append(image_files[-1])\n\n# Move the selected images to a new folder\nnew_folder_path = \"/kaggle/working/train_val_labels/100step/\"\nif not os.path.exists(new_folder_path):\n    os.makedirs(new_folder_path)\nfor image_file in selected_images:\n    shutil.copy(os.path.join(folder_path, image_file), new_folder_path)\n\nprint(f\"Selected and copied {len(selected_images)} images to {new_folder_path}\")","metadata":{"execution":{"iopub.execute_input":"2024-06-21T04:20:35.398339Z","iopub.status.busy":"2024-06-21T04:20:35.397414Z","iopub.status.idle":"2024-06-21T04:20:35.413100Z","shell.execute_reply":"2024-06-21T04:20:35.412149Z","shell.execute_reply.started":"2024-06-21T04:20:35.398297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ***2in1 (input_label)***","metadata":{}},{"cell_type":"code","source":"# Path to input and label folders\ninput_folder = \"/kaggle/working/train_val_inputs/100step/\"\nmask_folder = \"/kaggle/working/train_val_labels/100step/\"\n\n# Create a folder for the combined images\ngan_folder = \"/kaggle/working/train_val_data/100step/\"\nos.makedirs(gan_folder, exist_ok=True)\n\n# Get sorted lists of files in each folder\ninput_files = sorted([f for f in os.listdir(input_folder) if f.endswith('.png')])\nmask_files = sorted([f for f in os.listdir(mask_folder) if f.endswith('.png')])\n\n# Verify that both folders contain the same number of files\nif len(input_files) != len(mask_files):\n    raise ValueError(\"The number of input files and mask files do not match.\")\n\n# Loop through the 40\nfor i in range(min(40, len(input_files))):\n    input_image_path = os.path.join(input_folder, input_files[i])\n    mask_image_path = os.path.join(mask_folder, mask_files[i])\n\n    # Read the images in grayscale\n    input_image = cv2.imread(input_image_path, cv2.IMREAD_GRAYSCALE)\n    mask_image = cv2.imread(mask_image_path, cv2.IMREAD_GRAYSCALE)\n\n    # Check if both images are successfully loaded\n    if input_image is None or mask_image is None:\n        print(f\"Error loading images at index {i}: {input_image_path} or {mask_image_path}\")\n        continue\n\n    # Combine input and label images horizontally\n    combined_image = cv2.hconcat([input_image, mask_image])\n\n    # Save combined image to a folder\n    combined_image_path = os.path.join(gan_folder, f\"Bentheimer_{i:04d}.png\")\n    cv2.imwrite(combined_image_path, combined_image)\n\nshutil.rmtree(\"/kaggle/working/train_val_inputs\")\nshutil.rmtree(\"/kaggle/working/train_val_labels\")\nprint(f\"Processed {min(40, len(input_files))} images.\")","metadata":{"execution":{"iopub.execute_input":"2024-06-21T04:20:37.259926Z","iopub.status.busy":"2024-06-21T04:20:37.259132Z","iopub.status.idle":"2024-06-21T04:20:37.804198Z","shell.execute_reply":"2024-06-21T04:20:37.803216Z","shell.execute_reply.started":"2024-06-21T04:20:37.259895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ***Train & validation data preprocessing, augmentation and generators***","metadata":{}},{"cell_type":"code","source":"# Get the list of file paths for input images\nfile_paths = tf.data.Dataset.list_files(\"/kaggle/working/train_val_data/100step/*.png\")\n\n# Convert the dataset to a list\nfile_paths = list(file_paths.as_numpy_iterator())\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices(file_paths)","metadata":{"execution":{"iopub.execute_input":"2024-06-21T04:20:44.806393Z","iopub.status.busy":"2024-06-21T04:20:44.805741Z","iopub.status.idle":"2024-06-21T04:20:46.238571Z","shell.execute_reply":"2024-06-21T04:20:46.237557Z","shell.execute_reply.started":"2024-06-21T04:20:44.806366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining load function\ndef load(image_file):\n  # Read and decode an image file to a uint8 tensor\n  image = tf.io.read_file(image_file)\n  image = tf.io.decode_png(image)\n\n  # Split each image tensor into two tensors\n  w = tf.shape(image)[1]\n  w = w // 2\n  input = image[:, :w, :]\n  label = image[:, w:, :]\n\n  # Convert images to float16 tensors\n  input = tf.cast(input, tf.float32)\n  label = tf.cast(label, tf.float32)\n\n  return input, label\n\n# Defining normalize function\ndef normalize(input, label):\n    \n  # Normalize the input image by scaling its pixel values to the range [-1, 1]\n  input = (input / 127.5) - 1\n  \n  # Normalize the label image by scaling its pixel values to the range [-1, 1]\n  label = (label - 1.5) * (2/3)\n    \n  # Return the normalized input and label images\n  return input, label\n\n# Defining resize function\ndef r_size(input, label, height, width):\n    \n  # Resize the input image to the desired height and width using nearest neighbor interpolation\n  input = tf.image.resize(input, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    \n  # Resize the label image to the desired height and width using nearest neighbor interpolation\n  label = tf.image.resize(label, [height, width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    \n  # Return the resized input and real images\n  return input, label\n\n# Each image is 512x512 in size\nIMG_WIDTH = 512\nIMG_HEIGHT = 512\n\n# Defining random_crop function\ndef random_crop(input, label):\n    \n  # Stack the input and real images along the first dimension\n  stacked_image = tf.stack([input, label], axis=0)\n    \n  # Randomly crop the stacked image to the desired size\n  cropped_image = tf.image.random_crop(stacked_image, size=[2, IMG_WIDTH, IMG_HEIGHT, 1])\n    \n  # Return the cropped input and label images\n  return cropped_image[0], cropped_image[1]\n\n# Defining random_shear function\ndef random_shear(input, label, shear_range):\n    shear_factor = tf.random.uniform([], minval=-shear_range, maxval=shear_range)\n    transformed_input = tfa.image.transform(input, [1.0, shear_factor, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], fill_mode='reflect')\n    transformed_label = tfa.image.transform(label, [1.0, shear_factor, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], fill_mode='reflect')\n    return transformed_input, transformed_label\n\n# Defining random_zoom function\ndef random_zoom(input, label, zoom_range):\n    zoom_factor = tf.random.uniform([], minval=0.0, maxval=zoom_range) + 1.0\n    zoom_factor = tf.cast(zoom_factor, input.dtype)\n    cropped_height = tf.cast(tf.shape(input)[0], tf.float32) / zoom_factor\n    cropped_width = tf.cast(tf.shape(input)[1], tf.float32) / zoom_factor\n    central_fraction = tf.minimum(cropped_height / tf.cast(tf.shape(input)[0], tf.float32),\n                                  cropped_width / tf.cast(tf.shape(input)[1], tf.float32))\n    \n    cropped_input = tf.image.central_crop(input, central_fraction)\n    resized_input = tf.image.resize(cropped_input, tf.shape(input)[:2])\n    \n    cropped_label = tf.image.central_crop(label, central_fraction)\n    resized_label = tf.image.resize(cropped_label, tf.shape(label)[:2])\n    \n    return resized_input, resized_label\n\n# Defining random_jitter function\n@tf.function()\ndef random_jitter(input, label):\n  \n  # Resize the input and label images to (512+30)x(512+30)\n  input, label = r_size(input, label, IMG_WIDTH+30, IMG_WIDTH+30)\n\n  # Randomly crop the input and label images to 512x512\n  input, label = random_crop(input, label)\n\n  # Randomly flip the images left to right with 50% probability\n  if tf.random.uniform(()) > 0.5:\n    input = tf.image.flip_left_right(input)\n    label = tf.image.flip_left_right(label)\n\n  # Randomly flip the images up & down with 50% probability\n  if tf.random.uniform(()) > 0.5:\n    input = tf.image.flip_up_down(input)\n    label = tf.image.flip_up_down(label)\n    \n  # Randomly rotate the images with 50% probability\n  if tf.random.uniform(()) > 0.5:\n    input = tfa.image.rotate(input, tf.constant(np.pi/9), 'nearest', fill_mode='reflect')\n    label = tfa.image.rotate(label, tf.constant(np.pi/9), 'nearest', fill_mode='reflect')\n    \n  # Randomly zoom the images with 50% probability\n  if tf.random.uniform(()) > 0.5:\n    input, label = random_zoom(input, label, 0.1) \n\n  # Randomly shear the images with 50% probability\n  if tf.random.uniform(()) > 0.5:\n    input, label = random_shear(input, label, 0.1)\n    \n  # Return the randomly jittered input and label images\n  return input, label\n\n# Defining preparing function\ndef preparing(input, label):\n    \n  # Apply random jitter to the images\n  input, label = random_jitter(input, label)\n\n  # Return the preprocessed images\n  return input, label\n\n# Defining preparing training data function\ndef prep_train(image_file):\n  # Load the input and label images from the given image file\n  input, label = load(image_file)\n    \n  # Normalize the input and label images\n  input, label = normalize(input, label)\n    \n  # Preparing the  images\n  input, label = preparing(input, label)\n        \n  # Return the prepared images\n  return input, label\n\n# Defining preparing validation data function\ndef prep_val(image_file):\n  # Split the images from the given image file\n  input, label = load(image_file)\n    \n  # Normalize the images\n  input, label = normalize(input, label)\n    \n  # Return the prepared images\n  return input, label","metadata":{"execution":{"iopub.execute_input":"2024-06-21T04:20:53.332394Z","iopub.status.busy":"2024-06-21T04:20:53.332050Z","iopub.status.idle":"2024-06-21T04:20:53.357891Z","shell.execute_reply":"2024-06-21T04:20:53.356855Z","shell.execute_reply.started":"2024-06-21T04:20:53.332369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining generators\ndef generator(file_paths, batch_size, split_ratio, validation=False):\n    while True:\n        random.shuffle(file_paths)\n\n        train_paths = file_paths[:int(split_ratio * len(file_paths))]\n        val_paths = file_paths[int(split_ratio * len(file_paths)):]\n\n        try:\n            train_dataset = tf.data.Dataset.from_tensor_slices(train_paths)\n        except tf.errors.InvalidArgumentError:\n            train_dataset = tf.data.Dataset.from_tensor_slices(train_paths)\n        train_dataset = train_dataset.map(prep_train)\n        train_dataset = train_dataset.batch(batch_size)\n\n        try:\n            val_dataset = tf.data.Dataset.from_tensor_slices(val_paths)\n        except tf.errors.InvalidArgumentError:\n            val_dataset = tf.data.Dataset.from_tensor_slices(val_paths)\n        val_dataset = val_dataset.map(prep_val)\n        val_dataset = val_dataset.batch(batch_size)\n\n        if not validation:\n            for input, label in train_dataset:\n                \n                # [0 ,1]\n                input = (input + 1) / 2.0\n                input = np.array(input)\n                \n                # [0 ,3]\n                label = (label * 3/2) + 1.5\n                label = np.round(np.clip(label, 0, 3))\n                \n                label = to_categorical(\n                    label, num_classes=4)[:, :, :, 0:]\n\n                yield input, label\n\n        else:\n            for input, label in val_dataset:\n                \n                # [0, 1]\n                input = (input + 1) / 2.0\n                input = np.array(input)\n                \n                # [0 ,3]\n                label = (label * 3/2) + 1.5\n                label = np.round(np.clip(label, 0, 3))\n                \n                label = to_categorical(\n                    label, num_classes=4)[:, :, :, 0:]\n\n                yield input, label\n                \ntrain_generator = generator(file_paths, 8, 0.8, validation=False)\nval_generator = generator(file_paths, 8, 0.8, validation=True)","metadata":{"execution":{"iopub.execute_input":"2024-06-21T04:21:52.351608Z","iopub.status.busy":"2024-06-21T04:21:52.350702Z","iopub.status.idle":"2024-06-21T04:21:52.367239Z","shell.execute_reply":"2024-06-21T04:21:52.366166Z","shell.execute_reply.started":"2024-06-21T04:21:52.351563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***PCW***","metadata":{}},{"cell_type":"code","source":"# Get class counts from generator\nclass_counts = np.zeros(4)\nfor i in range(1000):\n    input, label = next(train_generator)\n    class_counts += np.sum(label, axis=(0, 1, 2))\n    \n# Get total count, initial & normalized class weights\ntotal_count = np.sum(class_counts)\nclass_weights = (total_count/(class_counts[0])),(total_count/(class_counts[1])),(total_count/(class_counts[2])),(total_count/(class_counts[3]))\ntotal_weight = sum(class_weights)\nnormalized_weights = [w/total_weight for w in class_weights]\nweights = [normalized_weights[0], normalized_weights[1], normalized_weights[2], normalized_weights[3]]\n\nprint(f\"total counts: {total_count}\")\nprint(f\"initial weights: {class_weights}\")\nprint(f\"normalized weights: {weights}\")","metadata":{"execution":{"iopub.execute_input":"2024-06-21T04:21:53.577002Z","iopub.status.busy":"2024-06-21T04:21:53.576630Z","iopub.status.idle":"2024-06-21T04:21:56.598595Z","shell.execute_reply":"2024-06-21T04:21:56.597758Z","shell.execute_reply.started":"2024-06-21T04:21:53.576974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PCW\ndef pcw(image, label):\n    class_weights = np.array(weights)\n    class_weights /= np.sum(class_weights)\n    sample_weights = np.zeros_like(label, dtype=np.float32)\n    for i in range(len(class_weights)):\n        sample_weights[label[..., i] == 1, i] = class_weights[i]\n\n    # Edge detection weighting\n    for idx in range(image.shape[0]):\n        img = image[idx] * 255\n        edges = cv2.Canny(img.astype(np.uint8), 100, 200)\n        edges = edges.astype(np.float32) / 255.0\n\n        # Distance-based weighting\n        for i in range(len(class_weights)):\n            distance = distance_transform_edt(label[idx, ..., i] == 1)\n            if np.max(distance) == 0:\n                distance_norm = np.ones_like(distance)\n            else:\n                distance_norm = distance / np.max(distance)\n            inverted_distance = 1 - distance_norm\n\n            # The sample weights\n            sample_weights[idx, ..., i] *= (1.0 + edges) * (inverted_distance) #4 Class frequency, edge detection, distance-based weighting\n            \n    # The final weights \n    final_weights = np.sum(sample_weights, axis=-1, keepdims=True)\n\n    return image, label, final_weights\n\n# Define weighted generators\ndef weighted_generator(generator):\n    for batch in generator:\n        # Apply the sample weights to the label batch\n        _, label = batch\n        _, _, sample_weights = pcw(_, label)\n        yield (batch[0], batch[1], sample_weights)\n\n# Create the weighted train and validation generators\nweighted_train_generator = weighted_generator(train_generator)\nweighted_val_generator = weighted_generator(val_generator)","metadata":{"execution":{"iopub.execute_input":"2024-06-21T04:24:20.170278Z","iopub.status.busy":"2024-06-21T04:24:20.169914Z","iopub.status.idle":"2024-06-21T04:24:20.183768Z","shell.execute_reply":"2024-06-21T04:24:20.182717Z","shell.execute_reply.started":"2024-06-21T04:24:20.170250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Modified U-Net***","metadata":{}},{"cell_type":"code","source":"# Defining weighted f1 score function\ndef weighted_f1(y_true, y_pred):\n    \"\"\"\n    Weighted F1 score metric function for Keras model\n    \"\"\"\n    # Convert predictions to binary values\n    y_pred = K.round(y_pred)\n\n    # Calculate true positives, false positives, and false negatives for each class\n    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\n\n    # Calculate precision and recall for each class\n    precision = tp / (tp + fp + K.epsilon())\n    recall = tp / (tp + fn + K.epsilon())\n\n    # Calculate F1 score for each class\n    f1_score = 2 * precision * recall / (precision + recall + K.epsilon())\n\n    # Calculate class weights based on their frequency in the dataset\n    class_weights = K.sum(y_true, axis=0) / K.sum(y_true)\n\n    # Calculate weighted F1 score\n    weighted_f1_score = K.sum(class_weights * f1_score)\n\n    return weighted_f1_score","metadata":{"execution":{"iopub.execute_input":"2024-06-21T05:40:04.624596Z","iopub.status.busy":"2024-06-21T05:40:04.623829Z","iopub.status.idle":"2024-06-21T05:40:04.632623Z","shell.execute_reply":"2024-06-21T05:40:04.631662Z","shell.execute_reply.started":"2024-06-21T05:40:04.624562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Oprimizer\noptimizer = Adam(learning_rate=1e-3)\n\n# Dynamic dropout rates\ndropout_rates = [0.1125, 0.125, 0.150, 0.2, 0.3, 0.2, 0.150, 0.125, 0.1125]\n\n# The convolutional block\ndef conv_block(inputs, n_filters, dropout_rate, kernel_size=3, batchnorm=True):\n    # first layer\n    conv = layers.Conv2D(n_filters, kernel_size=kernel_size, padding='same', \n                         dilation_rate=(1, 1), kernel_initializer='he_uniform',\n                         kernel_regularizer=regularizers.l2(1e-4), bias_regularizer=regularizers.l2(1e-4))(inputs)\n    conv = layers.BatchNormalization()(conv)\n    conv = layers.Activation('relu')(conv)\n    conv = layers.Dropout(dropout_rate)(conv)\n    return conv\n\n# The modified u_net model architecture\ndef get_model(input_shape, n_filters=16, kernel_size=3, batchnorm=True):\n    \n    # The input layer\n    inputs = layers.Input(input_shape)\n    \n    # Encoding part\n    conv1 = conv_block(inputs, 16*1, dropout_rates[0],  kernel_size=3, batchnorm=True)\n    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n    #####\n    conv2 = conv_block(pool1, 16*2, dropout_rates[1],  kernel_size=3, batchnorm=True)\n    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n    #####\n    conv3 = conv_block(pool2, 16*4, dropout_rates[2],  kernel_size=3, batchnorm=True)\n    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n    #####\n    conv4 = conv_block(pool3, 16*8, dropout_rates[3],  kernel_size=3, batchnorm=True)\n    pool4 = layers.MaxPooling2D(pool_size=(2, 2))(conv4)\n    #####\n    \n    # Bottom\n    conv5 = conv_block(pool4, 16*16, dropout_rates[4],  kernel_size=3, batchnorm=True)\n    \n    # Decoding part\n    up6 = layers.Conv2DTranspose(16*8, kernel_size=(3, 3), strides=(2, 2), padding='same')(conv5)\n    up6 = layers.concatenate([up6, conv4])\n    conv6 = conv_block(up6, 16*8, dropout_rates[5],  kernel_size=3, batchnorm=True)\n    #####\n    up7 = layers.Conv2DTranspose(16*4, kernel_size=(3, 3), strides=(2, 2), padding='same')(conv6)\n    up7 = layers.concatenate([up7, conv3])\n    conv7 = conv_block(up7, 16*4, dropout_rates[6],  kernel_size=3, batchnorm=True)\n    #####\n    up8 = layers.Conv2DTranspose(16*2, kernel_size=(3, 3), strides=(2, 2), padding='same')(conv7)\n    up8 = layers.concatenate([up8, conv2])\n    conv8 = conv_block(up8, 16*2, dropout_rates[7],  kernel_size=3, batchnorm=True)\n    #####\n    up9 = layers.Conv2DTranspose(16*1, kernel_size=(3, 3), strides=(2, 2), padding='same')(conv8)\n    up9 = layers.concatenate([up9, conv1])\n    conv9 = conv_block(up9, 16*1, dropout_rates[8],  kernel_size=3, batchnorm=True)\n    \n    # Classifier part\n    outputs = layers.Conv2D(4, kernel_size=(1, 1), activation='softmax')(conv9)\n    \n    # The model\n    model = models.Model(inputs=[inputs], outputs=[outputs])\n    return model\n\n# The input shape\ninput_shape = (512, 512, 1)\n\n# Get the model\nmodel = get_model(input_shape, n_filters=16, batchnorm=True)\n\n# Compliling the model\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', weighted_f1])","metadata":{"execution":{"iopub.execute_input":"2024-06-21T05:40:08.281502Z","iopub.status.busy":"2024-06-21T05:40:08.281096Z","iopub.status.idle":"2024-06-21T05:40:08.687718Z","shell.execute_reply":"2024-06-21T05:40:08.686903Z","shell.execute_reply.started":"2024-06-21T05:40:08.281469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate custom callbacks\n\n# 1 Define a callback to reduce learning rate for a specific situation\nrlr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, min_lr=1e-5, verbose=2)\n\n# 2 Define a callback to stop training process for a specific situation\nes = EarlyStopping(monitor='val_loss', mode='min', min_delta=1e-4, patience=8)\n\n# 3 Define a callback to save history logs\ncsvl = CSVLogger(\"./history_log.csv\", append=True)\n\n# 4 Define a callback to save the best weights\ncheckpoint_dir = \"./weights/\"\nif not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)\n\ncheckpoint_filepath = os.path.join(checkpoint_dir, \"best_weights.hdf5\")\nchp = ModelCheckpoint(checkpoint_filepath, monitor='loss', verbose=1, save_weights_only=True, save_best_only=True, mode='auto')","metadata":{"execution":{"iopub.execute_input":"2024-06-21T04:26:46.673466Z","iopub.status.busy":"2024-06-21T04:26:46.672819Z","iopub.status.idle":"2024-06-21T04:26:46.680974Z","shell.execute_reply":"2024-06-21T04:26:46.679813Z","shell.execute_reply.started":"2024-06-21T04:26:46.673422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Train & validation***","metadata":{}},{"cell_type":"code","source":"# Training & validation procedure\nweighted_train_generator = weighted_generator(train_generator)\nweighted_val_generator = weighted_generator(val_generator)\n\nnb_epoch = 1000\nbatch_size = 1\n\n# Train the model\nhistory = model.fit(\n    weighted_train_generator,\n    steps_per_epoch=1.6*len(file_paths) // batch_size,\n    \n    validation_data=weighted_val_generator,\n    validation_steps=0.4*len(file_paths) // batch_size,\n    \n    epochs=nb_epoch, verbose=1,\n        \n    callbacks=[ # Setting callbacks\n        csvl,\n        rlr,\n        chp,\n        es\n    ]\n    \n)","metadata":{"execution":{"iopub.execute_input":"2024-06-21T04:27:12.743699Z","iopub.status.busy":"2024-06-21T04:27:12.742887Z","iopub.status.idle":"2024-06-21T05:22:32.631563Z","shell.execute_reply":"2024-06-21T05:22:32.630595Z","shell.execute_reply.started":"2024-06-21T04:27:12.743670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the history of training and validation loss and accuracy\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']\ntrain_accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\n\n# Plot the loss\nplt.plot(train_loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend()\nplt.title('Loss')\nplt.show()\n\n# Plot the accuracy\nplt.plot(train_accuracy, label='Training Accuracy')\nplt.plot(val_accuracy, label='Validation Accuracy')\nplt.legend()\nplt.title('Accuracy')\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2024-06-21T05:25:11.507284Z","iopub.status.busy":"2024-06-21T05:25:11.506863Z","iopub.status.idle":"2024-06-21T05:25:12.097173Z","shell.execute_reply":"2024-06-21T05:25:12.096223Z","shell.execute_reply.started":"2024-06-21T05:25:11.507255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Evaluation & blind test***","metadata":{}},{"cell_type":"code","source":"# Due to RAM limits\nimport gc\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ***Load the best weight***","metadata":{}},{"cell_type":"code","source":"model.load_weights('/kaggle/working/weights/best_weights.hdf5')","metadata":{"execution":{"iopub.execute_input":"2024-06-21T05:40:15.956684Z","iopub.status.busy":"2024-06-21T05:40:15.955904Z","iopub.status.idle":"2024-06-21T05:40:16.066599Z","shell.execute_reply":"2024-06-21T05:40:16.065772Z","shell.execute_reply.started":"2024-06-21T05:40:15.956650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ***Preparing blind test sample c (also a & b)***","metadata":{}},{"cell_type":"code","source":"# Sample c (modify for evaluation of a & b)\ninput_dir = '/kaggle/working/sample_c/inputs/'\nmask_dir = '/kaggle/working/sample_c/labels/'\n\ninput_images = io.imread_collection(os.path.join(input_dir, '*.png'))\nmask_images = io.imread_collection(os.path.join(mask_dir, '*.png'))","metadata":{"execution":{"iopub.execute_input":"2024-06-21T05:40:17.794903Z","iopub.status.busy":"2024-06-21T05:40:17.794136Z","iopub.status.idle":"2024-06-21T05:40:17.858991Z","shell.execute_reply":"2024-06-21T05:40:17.858080Z","shell.execute_reply.started":"2024-06-21T05:40:17.794869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inputs\nx_test = []\nfor img in input_images:\n    img_norm = img.astype('float16') / 255.0\n    x_test.append(img_norm)\nx_test = np.array(x_test)","metadata":{"execution":{"iopub.execute_input":"2024-06-21T05:40:20.247166Z","iopub.status.busy":"2024-06-21T05:40:20.246248Z","iopub.status.idle":"2024-06-21T05:40:38.308802Z","shell.execute_reply":"2024-06-21T05:40:38.307940Z","shell.execute_reply.started":"2024-06-21T05:40:20.247131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Labels\ny_test = []\nfor img in mask_images:\n    img_onehot = to_categorical(img, num_classes=4)[:, :, 0:] # The background class (0) is excluded\n    y_test.append(img_onehot)\ny_test = np.array(y_test)\ny_test = y_test.astype('float16')","metadata":{"execution":{"iopub.execute_input":"2024-06-21T05:40:38.310801Z","iopub.status.busy":"2024-06-21T05:40:38.310498Z","iopub.status.idle":"2024-06-21T05:41:03.713685Z","shell.execute_reply":"2024-06-21T05:41:03.712556Z","shell.execute_reply.started":"2024-06-21T05:40:38.310774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving path\ntest_dir = \"/kaggle/working/test_data/\"\nif not os.path.exists(test_dir):\n    os.makedirs(test_dir)\n# Save the arrays\nnp.save(test_dir + '/x_test.npy', x_test)\nnp.save(test_dir + '/y_test.npy', y_test)","metadata":{"execution":{"iopub.execute_input":"2024-06-21T05:41:33.911142Z","iopub.status.busy":"2024-06-21T05:41:33.910233Z","iopub.status.idle":"2024-06-21T05:41:42.334676Z","shell.execute_reply":"2024-06-21T05:41:42.333774Z","shell.execute_reply.started":"2024-06-21T05:41:33.911107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test = np.load(test_dir + '/x_test.npy')\n\nbatch_size = 100\ny_pred = []\nfor i in range(0, len(x_test), batch_size):\n    batch_x_test = x_test[i:i+batch_size]\n    batch_y_pred = model.predict(batch_x_test)\n    y_pred.append(batch_y_pred)\n    print(i)\ny_pred = np.concatenate(y_pred, axis=0)\ny_pred = y_pred.astype('float16')\n\ndel x_test","metadata":{"execution":{"iopub.execute_input":"2024-06-21T06:33:09.341278Z","iopub.status.busy":"2024-06-21T06:33:09.340305Z","iopub.status.idle":"2024-06-21T06:34:03.370954Z","shell.execute_reply":"2024-06-21T06:34:03.369972Z","shell.execute_reply.started":"2024-06-21T06:33:09.341244Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ***Evaluation metrics***","metadata":{}},{"cell_type":"markdown","source":"### ***Pore***","metadata":{}},{"cell_type":"code","source":"y_test = np.load(test_dir + '/y_test.npy')\n\n# Compute confusion matrix for 1st class\ntrue = np.argmax(y_test, axis=-1) == 0\npred = np.argmax(y_pred, axis=-1) == 0\n\ndel y_test\n\ncm = confusion_matrix(true.ravel(), pred.ravel())\n\n# Normalize confusion matrix by true class frequencies\ncm_norm = cm / cm.sum(axis=1)[:, np.newaxis]\n\n# Plot confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_norm)\ndisp.plot(cmap = 'Blues')\ndisp.ax_.set_title('Confusion matrix: Macro pore space')\nplt.show()\n\naccuracy = (cm_norm[0, 0] + cm_norm[1, 1]) / np.sum(cm_norm)\n\nPrecision = cm_norm[1][1] / (cm_norm[1][1] + cm_norm[1][0])\nRecall = cm_norm[1][1] / (cm_norm[1][1] + cm_norm[0][1])\nF1 = 2 * (Precision * Recall) / (Precision + Recall)\niou = cm_norm[1, 1] / (cm_norm[1, 1] + cm_norm[0, 1] + cm_norm[1, 0])\n\nprint('Accuracy:', accuracy)\nprint('Precision:', Precision)\nprint('Recall:', Recall)\nprint('F1-Score:', F1)\nprint('Intersection over Union (IoU):', iou)","metadata":{"execution":{"iopub.execute_input":"2024-06-21T05:42:38.791112Z","iopub.status.busy":"2024-06-21T05:42:38.790336Z","iopub.status.idle":"2024-06-21T05:44:33.896684Z","shell.execute_reply":"2024-06-21T05:44:33.895633Z","shell.execute_reply.started":"2024-06-21T05:42:38.791074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ***Quartz***","metadata":{}},{"cell_type":"code","source":"# Compute confusion matrix for 2nd class\ny_test = np.load(test_dir + '/y_test.npy')\ntrue = np.argmax(y_test, axis=-1) == 1\ndel y_test\n\npred = np.argmax(y_pred, axis=-1) == 1\n\ncm = confusion_matrix(true.ravel(), pred.ravel())\n\n# Normalize confusion matrix by true class frequencies\ncm_norm = cm / cm.sum(axis=1)[:, np.newaxis]\n\n# Plot confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_norm)\ndisp.plot(cmap = 'Blues')\ndisp.ax_.set_title('Confusion matrix: Quartz')\nplt.show()\n\naccuracy = (cm_norm[0, 0] + cm_norm[1, 1]) / np.sum(cm_norm)\n\nPrecision = cm_norm[1][1] / (cm_norm[1][1] + cm_norm[1][0])\nRecall = cm_norm[1][1] / (cm_norm[1][1] + cm_norm[0][1])\nF1 = 2 * (Precision * Recall) / (Precision + Recall)\niou = cm_norm[1, 1] / (cm_norm[1, 1] + cm_norm[0, 1] + cm_norm[1, 0])\n\nprint('Accuracy:', accuracy)\nprint('Precision:', Precision)\nprint('Recall:', Recall)\nprint('F1-Score:', F1)\nprint('Intersection over Union (IoU):', iou)","metadata":{"execution":{"iopub.execute_input":"2024-06-21T05:44:33.899813Z","iopub.status.busy":"2024-06-21T05:44:33.899065Z","iopub.status.idle":"2024-06-21T05:46:29.745441Z","shell.execute_reply":"2024-06-21T05:46:29.744456Z","shell.execute_reply.started":"2024-06-21T05:44:33.899774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ***Clay***","metadata":{}},{"cell_type":"code","source":"# Compute confusion matrix for 3rd class\ny_test = np.load(test_dir + '/y_test.npy')\ntrue = np.argmax(y_test, axis=-1) == 2\ndel y_test\n\npred = np.argmax(y_pred, axis=-1) == 2\n\ncm = confusion_matrix(true.ravel(), pred.ravel())\n\n# Normalize confusion matrix by true class frequencies\ncm_norm = cm / cm.sum(axis=1)[:, np.newaxis]\n\n# Plot confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_norm)\ndisp.plot(cmap = 'Blues')\ndisp.ax_.set_title('Confusion matrix: Clay region')\nplt.show()\n\naccuracy = (cm_norm[0, 0] + cm_norm[1, 1]) / np.sum(cm_norm)\n\nPrecision = cm_norm[1][1] / (cm_norm[1][1] + cm_norm[1][0])\nRecall = cm_norm[1][1] / (cm_norm[1][1] + cm_norm[0][1])\nF1 = 2 * (Precision * Recall) / (Precision + Recall)\niou = cm_norm[1, 1] / (cm_norm[1, 1] + cm_norm[0, 1] + cm_norm[1, 0])\n\nprint('Accuracy:', accuracy)\nprint('Precision:', Precision)\nprint('Recall:', Recall)\nprint('F1-Score:', F1)\nprint('Intersection over Union (IoU):', iou)","metadata":{"execution":{"iopub.execute_input":"2024-06-21T05:46:29.747338Z","iopub.status.busy":"2024-06-21T05:46:29.747026Z","iopub.status.idle":"2024-06-21T05:48:23.934549Z","shell.execute_reply":"2024-06-21T05:48:23.933552Z","shell.execute_reply.started":"2024-06-21T05:46:29.747309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ***Feldspar***","metadata":{}},{"cell_type":"code","source":"# Compute confusion matrix for 4th class\ny_test = np.load(test_dir + '/y_test.npy')\ntrue = np.argmax(y_test, axis=-1) == 3\ndel y_test\n\npred = np.argmax(y_pred, axis=-1) == 3\n\ncm = confusion_matrix(true.ravel(), pred.ravel())\n\n# Normalize confusion matrix by true class frequencies\ncm_norm = cm / cm.sum(axis=1)[:, np.newaxis]\n\n# Plot confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_norm)\ndisp.plot(cmap = 'Blues')\ndisp.ax_.set_title('Confusion matrix: Feldspar')\nplt.show()\n\naccuracy = (cm_norm[0, 0] + cm_norm[1, 1]) / np.sum(cm_norm)\n\nPrecision = cm_norm[1][1] / (cm_norm[1][1] + cm_norm[1][0])\nRecall = cm_norm[1][1] / (cm_norm[1][1] + cm_norm[0][1])\nF1 = 2 * (Precision * Recall) / (Precision + Recall)\niou = cm_norm[1, 1] / (cm_norm[1, 1] + cm_norm[0, 1] + cm_norm[1, 0])\n\nprint('Accuracy:', accuracy)\nprint('Precision:', Precision)\nprint('Recall:', Recall)\nprint('F1-Score:', F1)\nprint('Intersection over Union (IoU):', iou)","metadata":{"execution":{"iopub.execute_input":"2024-06-21T05:48:23.936857Z","iopub.status.busy":"2024-06-21T05:48:23.936534Z","iopub.status.idle":"2024-06-21T05:50:15.583469Z","shell.execute_reply":"2024-06-21T05:50:15.582455Z","shell.execute_reply.started":"2024-06-21T05:48:23.936830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"slide_number = 0\n\n# Load inputs and labels\nx_test = np.load('/kaggle/working/test_data/x_test.npy')[slide_number]\ny_test = np.argmax(np.load('/kaggle/working/test_data/y_test.npy')[slide_number], axis=-1)\ny_pred0 = np.argmax(y_pred[slide_number], axis=-1)","metadata":{"execution":{"iopub.execute_input":"2024-06-21T06:36:51.041091Z","iopub.status.busy":"2024-06-21T06:36:51.040423Z","iopub.status.idle":"2024-06-21T06:37:11.984197Z","shell.execute_reply":"2024-06-21T06:37:11.983259Z","shell.execute_reply.started":"2024-06-21T06:36:51.041060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to plot\ndef plot_input_and_label(x_test, y_test, cmap='gray'):\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    axes[0].imshow(x_test, cmap=cmap)\n    axes[0].set_title(\"Input\")\n\n    axes[1].imshow(y_test, cmap=cmap)\n    axes[1].set_title(\"Label\")\n\n    # Legend for the Label image\n    legend_labels = [Patch(facecolor='black', label='Pore'),\n                    Patch(facecolor='gray', label='Quartz'),\n                    Patch(facecolor='darkgray', label='Clay'),\n                    Patch(facecolor='white', label='Feldspar')]\n    axes[1].legend(handles=legend_labels, loc='lower right', fontsize=10)\n    \n    # Add rectangle to the \"Label\" plot i.e., axes[1]\n    rect = Rectangle((2, y_test.shape[0] - 258), 257, 256, linewidth=2, edgecolor='r', facecolor='none')\n    axes[1].add_patch(rect)\n    \n    plt.tight_layout()\n    plt.show()\n    \nplot_input_and_label(x_test, y_test)","metadata":{"execution":{"iopub.execute_input":"2024-06-21T06:37:11.986290Z","iopub.status.busy":"2024-06-21T06:37:11.985990Z","iopub.status.idle":"2024-06-21T06:37:12.604573Z","shell.execute_reply":"2024-06-21T06:37:12.603645Z","shell.execute_reply.started":"2024-06-21T06:37:11.986265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_predictions_and_errors(x_test, y_test, y_pred, index=0):\n    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n    \n    # Normalize\n    norm = Normalize(vmin=0, vmax=1)\n    \n    # Plot Input\n    axes[0].imshow(x_test[256:512, :256], cmap='gray')\n    axes[0].set_title(\"Input\")\n    \n    # Plot Label\n    axes[1].imshow(y_test[256:512, :256], cmap='gray')\n    axes[1].set_title(\"True Label\")\n    \n    # Plot Prediction\n    axes[2].imshow(y_pred0[256:512, :256], cmap='gray')\n    axes[2].set_title(\"Prediction\")\n    \n    # Plot Error Map\n    error_map = np.abs(y_pred0 - y_test)\n    error_map = np.where(error_map > 0, 1, 0)\n    axes[3].imshow(error_map[256:512, :256], cmap='gray')\n    axes[3].set_title(\"Error Map\")\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_predictions_and_errors(x_test, y_test, y_pred, index=0)","metadata":{"execution":{"iopub.execute_input":"2024-06-21T06:46:43.740305Z","iopub.status.busy":"2024-06-21T06:46:43.739508Z","iopub.status.idle":"2024-06-21T06:46:44.847768Z","shell.execute_reply":"2024-06-21T06:46:44.846863Z","shell.execute_reply.started":"2024-06-21T06:46:43.740271Z"},"trusted":true},"execution_count":null,"outputs":[]}]}